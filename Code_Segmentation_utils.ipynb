{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 - save newline and space info (DONE, See below)\n",
    "#  a. - should this be in the tokenizer decode function\n",
    "#  b. - Did we need to add something to the vocab??\n",
    "#2 - add delineating NL between snippets (Done)\n",
    "#  a. - This is taken care of when we generate the aligned snippets\n",
    "#  b. - The need for a delineating new line seems like a drawback, any way to do it without one?\n",
    "#3 - generate aligned snippets (Moderate success, works most of the time it appears)\n",
    "#  a. - This comes down to making the sure the first token(s) of the samples are the same\n",
    "#  b. - for now this will be only the first token, but you could do it for sever, it works but much fewer have the same X-number of tokens, sometimes gets similar functions,\n",
    "#  c. - there are cases this doesn't cover, such as a class that has defined functions of its own (i.e., naturally occurring diff alignment)\n",
    "#  d. - takes longer than I expected??\n",
    "#  e. - making more than 1 alignment token can make it process a lot, sometimes get similar code interestingly\n",
    "#4 - create the data for LSTM and save it (1000)\n",
    "#  a. - initial param: window diameter 20\n",
    "#  b. - inconsistent number of items per window!?\n",
    "\n",
    "#other:\n",
    "#-------------------------------------------------\n",
    "#language related things, importance of whitespace\n",
    "#https://en.wikipedia.org/wiki/Whitespace_(programming_language)\n",
    "#https://en.wikipedia.org/wiki/Brainfuck\n",
    "\n",
    "#what about size of input for training the tokenizer\n",
    "\n",
    "#issue with ‚ñÅ token\n",
    "\n",
    "#are AST tokens akin to named entity recognition or semantic role labeling... i guess it would be syntactic role labeling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num samples:  1278738\n",
      "num lines:  7729264\n"
     ]
    }
   ],
   "source": [
    "# open file in read mode\n",
    "number_of_lines = 0\n",
    "num_chars = 0 #TODO\n",
    "with open('code-comment-300-300.csv', 'r') as read_obj:\n",
    "    # pass the file object to reader() to get the reader object\n",
    "    csv_reader = reader(read_obj)\n",
    "    # Iterate over each row in the csv using reader object\n",
    "    \n",
    "    data = []\n",
    "    for row in csv_reader:\n",
    "        # row variable is a list that represents a row in csv\n",
    "        #row[0] - code\n",
    "        #row[1] - comment\n",
    "        \n",
    "        number_of_lines+=row[0].count('\\n')\n",
    "        \n",
    "        #way to handle white spaces:\n",
    "        #space #do first, \n",
    "        new_code = row[0].replace(' ',' SPACE')\n",
    "        #newline\n",
    "        new_code = new_code.replace('\\n',' NEWLINE')\n",
    "        #tab\n",
    "        new_code = new_code.replace('\\t',' TAB')\n",
    "        \n",
    "        #TODO\n",
    "        #FILTER FOR UNICODE THING\n",
    "        \n",
    "        \n",
    "        #save new data\n",
    "        row = [new_code,row[1]]\n",
    "        data.append(row)\n",
    "    \n",
    "    read_obj.close()\n",
    "    \n",
    "print('num samples: ', len(data))\n",
    "print('num lines: ', number_of_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pypi.org/project/sentencepiece/\n",
    "#http://ethen8181.github.io/machine-learning/deep_learning/subword/bpe.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.txt', 'w') as filehandle:\n",
    "    for i in range(len(data)):\n",
    "        filehandle.write('%s\\n' % data[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "\n",
    "    def __init__(self, filepath='python_tokenizer_30k.model'):\n",
    "        self.sp = spm.SentencePieceProcessor(model_file=filepath)\n",
    "\n",
    "    def encode(self, text, t=int):\n",
    "        return self.sp.encode(text, out_type=t)\n",
    "\n",
    "    def decode(self, pieces):\n",
    "        return self.sp.decode(pieces)\n",
    "\n",
    "    @staticmethod\n",
    "    def train(input_file='data/raw_sents.txt', model_prefix='sp_model', vocab_size=30522):\n",
    "        spm.SentencePieceTrainer.train(input=input_file, model_prefix=model_prefix, vocab_size=vocab_size,\n",
    "                                       #input_sentence_size=2 ** 16, shuffle_input_sentence=True)\n",
    "                                       input_sentence_size=number_of_lines, shuffle_input_sentence=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "Tokenizer.train(input_file='data.txt', model_prefix='python_tokenizer_30k', vocab_size=30000) #model_prefix is model storage name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate tokenizer model\n",
    "tokenizer = Tokenizer('python_tokenizer.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' SPACE SPACE SPACE SPACEdef SPACEunlock_and_close(self): NEWLINE SPACE SPACE SPACE SPACE SPACE SPACE NEWLINE SPACE SPACE SPACE SPACE SPACE SPACEif SPACEself._locked: NEWLINE SPACE SPACE SPACE SPACE SPACE SPACE SPACE SPACEfcntl.lockf(self._fh.fileno(), SPACEfcntl.LOCK_UN) NEWLINE SPACE SPACE SPACE SPACE SPACE SPACEself._locked SPACE= SPACEFalse NEWLINE SPACE SPACE SPACE SPACE SPACE SPACEif SPACEself._fh: NEWLINE SPACE SPACE SPACE SPACE SPACE SPACE SPACE SPACEself._fh.close() NEWLINE'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see some tokenized data (code)\n",
    "data[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 3, 13, 3, 3565, 5, 303, 5, 255, 6, 8, 10, 4, 3, 3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 24, 3, 8, 21, 2328, 15, 4, 3, 3, 3, 3, 3, 3, 3, 3, 4818, 7, 529, 73, 6, 8, 21, 2040, 7, 3658, 336, 3, 4818, 7, 4693, 5, 1826, 12, 4, 3, 3, 3, 3, 3, 3, 8, 21, 2328, 3, 11, 3, 72, 4, 3, 3, 3, 3, 3, 24, 3, 8, 21, 2040, 15, 4, 3, 3, 3, 3, 3, 3, 3, 3, 8, 21, 2040, 7, 255, 19, 4]\n"
     ]
    }
   ],
   "source": [
    "#tokenize code (ie encode)\n",
    "#with words\n",
    "#tokens = tokenizer.encode(data[1][0],t=str)\n",
    "#with numbers\n",
    "tokens = tokenizer.encode(data[1][0])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       def  unlock_and_close(self): \n",
      "             \n",
      "            if  self._locked: \n",
      "                fcntl.lockf(self._fh.fileno(),  fcntl.LOCK_UN) \n",
      "            self._locked  =  False \n",
      "            if  self._fh: \n",
      "                self._fh.close() \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#detokenize\n",
    "def decode_tokenized_code_snippet(tokens):\n",
    "    decoded = tokenizer.decode(tokens)\n",
    "    #decode but still has the added strings\n",
    "    #print(decoded)\n",
    "    token_string = ''.join(decoded)\n",
    "    token_string = token_string.replace('SPACE',' ')\n",
    "    token_string = token_string.replace('NEWLINE','\\n')\n",
    "    token_string = token_string.replace('TAB','\\t')\n",
    "    #print(token_string)\n",
    "    return token_string\n",
    "\n",
    "print(decode_tokenized_code_snippet(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from random import sample\n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[depricated]: see generate_verified_code_from_data  \n",
    "def generate_code_from_data(data,num_samples):\n",
    "    # \n",
    "    code_indixes = list(range(1,len(data)))\n",
    "    sampled_indexes = sample(code_indixes,num_samples)\n",
    "\n",
    "    generated_code = []\n",
    "    generated_code_ids = []\n",
    "    break_locations = []\n",
    "    for i in sampled_indexes:\n",
    "        \n",
    "        #get raw code\n",
    "        sampled_code_snippet = data[i][0] #just the code, thus the 0\n",
    "        \n",
    "        #tokenize it \n",
    "        #with words\n",
    "        sampled_code_snippet = tokenizer.encode(data[i][0],t=str)\n",
    "        #with numbers\n",
    "        sampled_code_snippet_ids = tokenizer.encode(data[i][0])\n",
    "        \n",
    "        #first break\n",
    "        if len(break_locations)==0:\n",
    "            break_locations.append(len(sampled_code_snippet))\n",
    "        \n",
    "        #add length from the previous one\n",
    "        else:\n",
    "            break_locations.append(len(sampled_code_snippet)+break_locations[-1])\n",
    "            \n",
    "        #concantonate the code sample\n",
    "        generated_code+=sampled_code_snippet\n",
    "        generated_code_ids+=sampled_code_snippet_ids\n",
    "    \n",
    "    return generated_code, break_locations, generated_code_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "code, breaks, _ = generate_code_from_data(data,3)\n",
    "#do it with the code tokens\n",
    "wd=20 #window diameter\n",
    "X_windows = centered_sliding_window(code,wd,encode=True)\n",
    "#print(X_windows)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[depricated]: see generate_verified_code_from_data  \n",
    "\n",
    "#note: it seems \"harder\" to generate these aligned snippets than I expected\n",
    "def generate_aligned_code_from_data(data,num_samples):\n",
    "    \n",
    "    #get all indixes of code\n",
    "    code_indixes = list(range(1,len(data)))\n",
    "    \n",
    "    tokenized_code = []\n",
    "    #generated_code_ids = []\n",
    "    break_locations = []\n",
    "    \n",
    "    #pick a random snippet\n",
    "    first_index = random.choice(code_indixes)\n",
    "    first_sampled_code_snippet = data[first_index][0] #just the code, thus the 0\n",
    "    \n",
    "    #tokenize it \n",
    "    #with words\n",
    "    #tokenized_code_snippet = tokenizer.encode(data[i][0],t=str)\n",
    "    tokenized_code_snippet = tokenizer.encode(first_sampled_code_snippet,t=str)\n",
    "    #with numbers\n",
    "    #sampled_code_snippet_ids = tokenizer.encode(data[i][0])\n",
    "    \n",
    "    #THIS IS HOW WE ALIGN IT??? CAUSE WHY?\n",
    "    #FIRST TOKEN\n",
    "    #just the first one\n",
    "    alignment_token = tokenized_code_snippet[0]\n",
    "    #first x\n",
    "    #x=2\n",
    "    #alignment_token = tokenized_code_snippet[:x]\n",
    "    \n",
    "    #save first snippet to the generated tokenized code\n",
    "    tokenized_code+=tokenized_code_snippet\n",
    "    \n",
    "    #pick more random snippets until we find two more with same first token\n",
    "    #find other snippets with same first alingment \n",
    "    while len(break_locations)<num_samples-1:\n",
    "        \n",
    "        #get new random candidate\n",
    "        new_index = random.choice(code_indixes)\n",
    "        candidate_next_snippet = data[new_index][0]\n",
    "        \n",
    "        #do i need to make sure it doesnt repeat itself??\n",
    "        \n",
    "        #tokenize it \n",
    "        #with words\n",
    "        tokenized_candidate = tokenizer.encode(candidate_next_snippet,t=str)\n",
    "        if tokenized_candidate[0]==alignment_token:\n",
    "        #if tokenized_candidate[:x]==alignment_token:\n",
    "            \n",
    "            #add delinating token before adding new snippet tokens\n",
    "            #If there is no new line at end of snippet\n",
    "            if tokenized_code[-1][-7:]!='NEWLINE':\n",
    "                tokenized_code+=[' NEWLINE']\n",
    "            #save break location\n",
    "            break_locations.append(len(tokenized_code)-1)\n",
    "            tokenized_code+=tokenized_candidate\n",
    "            #print('good--')\n",
    "            #-1 right?\n",
    "            \n",
    "        #not alinged\n",
    "        else:\n",
    "            pass\n",
    "            #print('bad?')\n",
    "            \n",
    "    return tokenized_code, break_locations\n",
    "        \n",
    "    \n",
    "#generate_aligned_code_from_data(data,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "code, breaks = generate_aligned_code_from_data(data,3)\n",
    "#do it with the code tokens\n",
    "wd=20 #window diameter\n",
    "X_windows = centered_sliding_window(code,wd,encode=True)\n",
    "#print(X_windows)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#Alignment Utils for generate_aligned_code_from_data_2 [Depricated]\n",
    "\n",
    "#find the number of those tokens at the begining that goes uninterupted\n",
    "def find_alignment_series(l):\n",
    "    #find first token\n",
    "    first_token = l[0]\n",
    "    \n",
    "    #find how many of these tokens are a the begining\n",
    "    num = 1\n",
    "    uninterupted=True\n",
    "    while uninterupted == True:\n",
    "        if l[num]==first_token:\n",
    "            num+=1\n",
    "        else:\n",
    "            uninterupted=False\n",
    "    return [first_token]*num\n",
    "    \n",
    "#additional snippets are aligned if they also have that many uninterupted tokens\n",
    "def check_alignment(a,b):\n",
    "    return len(find_alignment_series(a)) == len(find_alignment_series(b))\n",
    "    \n",
    "    \n",
    "a = ['a','a','a','b','a','b','c'] #first list\n",
    "#alignment should be ['a','a','a']\n",
    "\n",
    "b = ['a','b','a','b','c'] #not aligned\n",
    "c = ['a','a','a','c']     #aligned\n",
    "\n",
    "alignment_series = find_alignment_series(a)\n",
    "print(check_alignment(a,b))\n",
    "print(check_alignment(a,c))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[depricated]: see generate_verified_code_from_data  \n",
    "\n",
    "#Alignment handeled by generic string analizer\n",
    "def generate_aligned_code_from_data_2(data,num_samples):\n",
    "    \n",
    "    #get all indixes of code\n",
    "    code_indixes = list(range(1,len(data)))\n",
    "    \n",
    "    tokenized_code = []\n",
    "    break_locations = []\n",
    "    \n",
    "    #pick a random snippet\n",
    "    first_index = random.choice(code_indixes)\n",
    "    first_sampled_code_snippet = data[first_index][0] #just the code, thus the 0\n",
    "    first_tokenized_code_snippet = tokenizer.encode(first_sampled_code_snippet,t=str)\n",
    "    tokenized_code+=first_tokenized_code_snippet\n",
    "    \n",
    "    #print('original:',find_alignment_series(first_tokenized_code_snippet))\n",
    "    \n",
    "    #pick more random snippets until we find two more with same first token\n",
    "    #find other snippets with same first alingment \n",
    "    while len(break_locations)<num_samples-1:\n",
    "        \n",
    "        #get new random candidate\n",
    "        ###do i need to make sure it doesnt repeat itself??\n",
    "        new_index = random.choice(code_indixes)\n",
    "        candidate_next_snippet = data[new_index][0]\n",
    "        #tokenize it with words\n",
    "        tokenized_candidate = tokenizer.encode(candidate_next_snippet,t=str)\n",
    "        #print(find_alignment_series(tokenized_candidate))\n",
    "        \n",
    "        if check_alignment(first_tokenized_code_snippet,tokenized_candidate):\n",
    "            #print(find_alignment_series(tokenized_candidate))\n",
    "            \n",
    "            #add delinating token before adding new snippet tokens\n",
    "            #If there is no new line at end of snippet\n",
    "            if tokenized_code[-1][-7:]!='NEWLINE':\n",
    "                tokenized_code+=[' NEWLINE']\n",
    "            \n",
    "            #save break location\n",
    "            break_locations.append(len(tokenized_code)-1)\n",
    "            tokenized_code+=tokenized_candidate\n",
    "            #print('good')\n",
    "            \n",
    "        #not alinged\n",
    "        else:\n",
    "            pass\n",
    "            #print('bad?')\n",
    "            \n",
    "    return tokenized_code, break_locations\n",
    "        \n",
    "    \n",
    "#generate_aligned_code_from_data_2(data,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "code, breaks = generate_aligned_code_from_data_2(data,3)\n",
    "#do it with the code tokens\n",
    "wd=20 #window diameter\n",
    "X_windows = centered_sliding_window(code,wd,encode=True)\n",
    "#print(X_windows)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "#from Tokenize_python\n",
    "#######\n",
    "import ast\n",
    "import tokenize\n",
    "import io\n",
    "import numpy as np\n",
    "from tqdm import tqdm #inline progress bar (quality of life)\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nv=0\\nnv=0\\nfor d in data:\\n    snip = d[0]\\n    if check_code_validity(snip):\\n        v+=1\\n    else:\\n        nv+=1\\nprint((v/(v+nv))*100)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_code_validity(code_snippet):\n",
    "    valid=False\n",
    "    try:\n",
    "        code_snippet = decode_tokenized_code_snippet(code_snippet)\n",
    "        tokens =  tokenize.tokenize(io.BytesIO(code_snippet.encode('utf-8')).readline)\n",
    "        for t in tokens:\n",
    "            #this is enough to trigger it\n",
    "            pass\n",
    "        valid=True\n",
    "    except Exception as e:\n",
    "        pass\n",
    "        #print(e)\n",
    "    return valid\n",
    "    \n",
    "\n",
    "#prints percent valid snippets in data\n",
    "'''\n",
    "v=0\n",
    "nv=0\n",
    "for d in data:\n",
    "    snip = d[0]\n",
    "    if check_code_validity(snip):\n",
    "        v+=1\n",
    "    else:\n",
    "        nv+=1\n",
    "print((v/(v+nv))*100)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_tokenized_snippet(data,code_indixes):\n",
    "    i = random.choice(code_indixes)\n",
    "    snip = data[i][0] #just the code, thus the 0\n",
    "    tokenized_snip =  tokenizer.encode(snip,t=str)\n",
    "    return tokenized_snip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alinging by validing with AST\n",
    "def generate_verified_code_from_data(data,num_samples):\n",
    "    \n",
    "    #get all indixes of code\n",
    "    code_indixes = list(range(1,len(data)))\n",
    "    \n",
    "    #prep data\n",
    "    tokenized_code = []\n",
    "    break_locations = []\n",
    "    \n",
    "    #valid segment search params\n",
    "    trys = 0\n",
    "    max_trys = 100\n",
    "    \n",
    "    \n",
    "    #pick more random snippets until we find two more with same first token\n",
    "    #find other snippets with same first alingment \n",
    "    while len(break_locations)<num_samples:\n",
    "        \n",
    "        \n",
    "        #print('----- inside while, before getting new cadidate code')\n",
    "        #print(decode_tokenized_code_snippet(tokenizer.decode(tokenized_code)))\n",
    "        #input()\n",
    "        \n",
    "        #get new random candidate\n",
    "        ###do i need to make sure it doesnt repeat itself??\n",
    "        new_index = random.choice(code_indixes)\n",
    "        candidate_next_snippet = data[new_index][0]\n",
    "        #tokenize it with words\n",
    "        tokenized_candidate=[]\n",
    "        tokenized_candidate = tokenizer.encode(candidate_next_snippet,t=str)\n",
    "        #print(find_alignment_series(tokenized_candidate))\n",
    "        \n",
    "        #print('----- got tokenized candidate code')\n",
    "        #print(decode_tokenized_code_snippet(tokenizer.decode(tokenized_code)))\n",
    "        #input()\n",
    "        \n",
    "        \n",
    "        #if temp_tokenized_code[-1][-7:]!='NEWLINE':\n",
    "        #    temp_tokenized_code+=['NEWLINE']\n",
    "        if len(tokenized_code)>1 and tokenized_code[-1][-8:]!='‚ñÅNEWLINE':\n",
    "            #print('here')\n",
    "            tokenized_code+=['‚ñÅNEWLINE']\n",
    "        \n",
    "        temp_tokenized_code = tokenized_code + tokenized_candidate\n",
    "        \n",
    "        #print('----- made the temporaty tokenized code to check')\n",
    "        #print(decode_tokenized_code_snippet(tokenizer.decode(tokenized_code)))\n",
    "        #input()\n",
    "        \n",
    "        #see if it is aligned (ie a valid code snippet)\n",
    "        if check_code_validity(temp_tokenized_code):\n",
    "            #print('good')\n",
    "            tokenized_code=temp_tokenized_code\n",
    "            break_locations.append(len(tokenized_code)-1)\n",
    "                \n",
    "            #print('----- just added new peice ')\n",
    "            #print(decode_tokenized_code_snippet(tokenizer.decode(tokenized_code)))\n",
    "            \n",
    "                  \n",
    "        else:\n",
    "            #reset the test snippet\n",
    "            temp_tokenized_code=[]\n",
    "            trys+=1\n",
    "            #print('bad',trys)\n",
    "            #print(len(temp_tokenized_code))\n",
    "        #print('-----')\n",
    "        #print(decode_tokenized_code_snippet(tokenizer.decode(tokenized_code)))\n",
    "        #print(len(tokenized_code))\n",
    "        #input()\n",
    "        \n",
    "        \n",
    "        #if we our snippet is hard to pair\n",
    "        if trys>=max_trys:\n",
    "            tokenized_code=[]\n",
    "            break_locations=[]\n",
    "            trys=0\n",
    "            #print('restart')\n",
    "    \n",
    "    #we dont want to include the last break location, only ones between code segments\n",
    "    return tokenized_code, break_locations[:-1]\n",
    "\n",
    "#generate_aligned_code_from_data_3(data, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put comment in at break points\n",
    "#https://www.tutorialspoint.com/python/list_insert.htm\n",
    "#list.insert(index, obj)\n",
    "def insert_comments(code, break_spots, comment='\\n'+'*'*8+'\\n',at_begining=True):\n",
    "    #if there is a a comment at begining of snippet\n",
    "    if at_begining:\n",
    "        #adds a notation to add a 0\n",
    "        #at beigning of break spots too\n",
    "        break_spots.insert(0,0)\n",
    "    \n",
    "    \n",
    "    #go through breaks backwards\n",
    "    #so as not to mess up break \n",
    "    #spots as we would if we went forward\n",
    "    for b in break_spots[::-1]:\n",
    "        code.insert(b,comment)\n",
    "    return code\n",
    "\n",
    "\n",
    "#code=['a','b','c','d']\n",
    "#c = insert_comments(code,[2])\n",
    "#print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "code:\n",
      "--------------------------------\n",
      "       def  test_api_prefix(self): \n",
      "                 \n",
      "                self.test_app( \n",
      "                        options={ \n",
      "                                'pyramid_jsonapi.route_pattern_api_prefix':  'api' \n",
      "                        }).get('/api/people') \n",
      " \n",
      "                self.direct_control  =  False \n",
      "                self.paths_to_delete  =  [] \n",
      "\n",
      "--------------------------------\n",
      "break spots:  [108, 126]\n",
      "--------------------------------\n",
      "code with break spots indicated:\n",
      "--------------------------------\n",
      "\n",
      "********\n",
      "        def  test_api_prefix(self): \n",
      "                 \n",
      "                self.test_app( \n",
      "                        options={ \n",
      "                                'pyramid_jsonapi.route_pattern_api_prefix':  'api' \n",
      "                        }).get('/api/people') \n",
      "\n",
      "********\n",
      " \n",
      "                self.direct_control  =  False\n",
      "********\n",
      " \n",
      "                self.paths_to_delete  =  [] \n",
      "\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "####ALIGNMENT ISSUES, UNCOMMENT THE 'good' AND 'bad' PRINT,\n",
    "####SEE SOEMTIMES WHEN UNICODE CHAR IS THERE IT MESSES UP\n",
    "####\n",
    "\n",
    "\n",
    "#code, breaks, _ = generate_code_from_data(data,3)\n",
    "#code, breaks = generate_aligned_code_from_data_2(data,3)\n",
    "code, breaks = generate_verified_code_from_data(data,3)\n",
    "print('--------------------------------')\n",
    "print('code:')\n",
    "print('--------------------------------')\n",
    "decoded = tokenizer.decode(code)\n",
    "token_string = ''.join(decoded)\n",
    "token_string = token_string.replace('SPACE',' ')\n",
    "token_string = token_string.replace('NEWLINE','\\n')\n",
    "token_string = token_string.replace('TAB','\\t')\n",
    "print(token_string)\n",
    "#print(code)\n",
    "print('--------------------------------')\n",
    "print('break spots: ',breaks)\n",
    "print('--------------------------------')\n",
    "print('code with break spots indicated:')\n",
    "print('--------------------------------')\n",
    "#insert_comments\n",
    "comments_added = insert_comments(code,breaks)\n",
    "comments_added_decoded = tokenizer.decode(comments_added)\n",
    "comments_added_token_string = ''.join(comments_added_decoded)\n",
    "comments_added_token_string = comments_added_token_string.replace('SPACE',' ')\n",
    "comments_added_token_string = comments_added_token_string.replace('NEWLINE','\\n')\n",
    "comments_added_token_string = comments_added_token_string.replace('TAB','\\t')\n",
    "print(comments_added_token_string)\n",
    "print('--------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centered_sliding_window(token_list, window_diamiter,encode=False,PAD='unk'):\n",
    "    windows = []\n",
    "    for i in range(len(token_list)):\n",
    "        \n",
    "        #print(token_list)\n",
    "        #input()\n",
    "        \n",
    "        window = []\n",
    "        \n",
    "        #if we have to pad the begining\n",
    "        if i < window_diamiter:\n",
    "            before_len = window_diamiter-i\n",
    "            before = [PAD]*before_len+token_list[0:i]\n",
    "        else:\n",
    "            before = token_list[i-window_diamiter:i]\n",
    "        \n",
    "        #if we have to pad the end\n",
    "        if i+window_diamiter>=len(token_list):\n",
    "            after_len = (i+1+window_diamiter)-len(token_list)\n",
    "            after = token_list[i+1:i+1+window_diamiter]+[PAD]*after_len\n",
    "\n",
    "        else:\n",
    "            after = token_list[i+1:i+1+window_diamiter]\n",
    "        \n",
    "        #put it togeather\n",
    "        #print('------')\n",
    "        #print('before:',before)\n",
    "        #print('center:',token_list[i])\n",
    "        #print('after:',after)\n",
    "        window = before + [token_list[i]] + after\n",
    "        #for encoding code if we want\n",
    "        if encode:\n",
    "            new_window = []\n",
    "            #print(window)\n",
    "            #input()\n",
    "            for i in window:\n",
    "                encoded = tokenizer.encode(i)\n",
    "                if len(encoded)>1:       \n",
    "                    x=encoded[1]\n",
    "                    if type(x)==list:\n",
    "                        new_window.append(x[0])\n",
    "                    else:\n",
    "                        new_window.append(x)\n",
    "                elif len(encoded)==1:\n",
    "                    if type(encoded)==list:\n",
    "                        new_window.append(encoded[0])\n",
    "                    else:\n",
    "                        new_window.append(encoded)\n",
    "                else:\n",
    "                    #for some reason it finds the unicode stuff __\n",
    "                    pass\n",
    "                    #print(window)\n",
    "                    #print(i)\n",
    "                    #print(encoded)\n",
    "                    #input()\n",
    "            #print(window)\n",
    "            #print(len(window))\n",
    "            #print(len(tokenizer.decode(window)))\n",
    "            #print(tokenizer.decode(window))\n",
    "            #print(len(tokenizer.encode(window)))\n",
    "            #window = tokenizer.encode(tokenizer.decode(window))\n",
    "            window = new_window\n",
    "        #print(window)\n",
    "        #print(len(window))\n",
    "        #input()\n",
    "\n",
    "        #save windowz\n",
    "        windows.append(window)\n",
    "    \n",
    "    return windows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(201)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "center: a - window:  ['unk', 'unk', 'unk', 'a', 'b', 'c', 'd']\n",
      "center: b - window:  ['unk', 'unk', 'a', 'b', 'c', 'd', 'e']\n",
      "center: c - window:  ['unk', 'a', 'b', 'c', 'd', 'e', 'unk']\n",
      "center: d - window:  ['a', 'b', 'c', 'd', 'e', 'unk', 'unk']\n",
      "center: e - window:  ['b', 'c', 'd', 'e', 'unk', 'unk', 'unk']\n"
     ]
    }
   ],
   "source": [
    "#example\n",
    "wd=3#window diameter\n",
    "\n",
    "l = ['a','b','c','d','e']#,'f','g','h','i','j','k','l']\n",
    "windows = centered_sliding_window(l,wd,encode=False)\n",
    "for w in windows:\n",
    "    #print(len(w))\n",
    "    print('center:',w[int(len(w)/2)],'- window: ',w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_points():\n",
    "    #code, breaks, _ = generate_code_from_data(data,3)\n",
    "    #code, breaks = generate_aligned_code_from_data_2(data,3)\n",
    "    code, breaks = generate_verified_code_from_data(data,3)\n",
    "    \n",
    "    #do it with the code tokens\n",
    "    wd=20 #window diameter\n",
    "    X_windows = centered_sliding_window(code,wd,encode=True)\n",
    "    \n",
    "    #do it with the ground truth \n",
    "    y = [0]*len(code)\n",
    "    for b in breaks:\n",
    "        y[b] = 1\n",
    "    Y_windows = centered_sliding_window(y,wd,encode=False,PAD=0)\n",
    "    #print(Y_windows)\n",
    "    \n",
    "    return X_windows, Y_windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#code, breaks = generate_aligned_code_from_data_2(data,3)\n",
    "code, breaks = generate_verified_code_from_data(data,3)\n",
    "y = [0]*len(code)\n",
    "for b in breaks:\n",
    "    y[b] = 1\n",
    "Y_windows = centered_sliding_window(y,wd,encode=False, PAD=0)\n",
    "#print(len(Y_windows))\n",
    "#print(Y_windows)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [14:00<00:00, 11.89it/s]\n"
     ]
    }
   ],
   "source": [
    "#SAVE DATA FOR BiLSTM\n",
    "\n",
    "import json \n",
    "\n",
    "def make_generated_code_dataset(num_snippets):\n",
    "    # Data to be written \n",
    "    LSMT_DATA = {}\n",
    "    for i in tqdm(range(num_snippets)):\n",
    "        x,y = make_data_points()\n",
    "        LSMT_DATA[str(i)] = {'x':x,'y':y}\n",
    "\n",
    "    with open(\"LSTM_DATA.json\", \"w\") as outfile: \n",
    "        json.dump(LSMT_DATA, outfile)\n",
    "        \n",
    "make_generated_code_dataset(10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
